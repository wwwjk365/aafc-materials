---
title: "Introductions to pbdR"
author: Michael Yan
output:
  revealjs::revealjs_presentation:
    theme: simple
    highlight: tango
    center: true
    smart: true
    incremental: true
    self_contained: true
---

## Batch Execution

- Running a serial R program in batch: 
```r
Rscript my_script.r
# or
R CMD BATCH my_script.R
```

- Running a parallel (with MPI) R program in batch
```r
mpirun -np 2 Rscript my_par_script.r
```

## Single Program/Multiple Data (SPMD)

- SPMD is arguably the simplest extension of serial programming
- Only one program is written, executed in batch on all processors
- Different processors are autonomous; there is no manager
- Dominant programming model for large machines for 30 years


## Introduction to pbdMPI

- MPI: Standard for managing communications (data and instructions) between different nodes/computers. 
- Implementations: OpenMPI, MPICH2, Cray MPT, ...
- Enables parallelism (via communication) on distributed machines
- Communicator: manages communications between processors 

---

The general process for directly — or indirectly — utilizing MPI in SPMD programs goes something like this: 

1. Initialize communicator(s). 
2. Have each process read in its portion of the data. 
3. Perform computations. 
4. Communicate results. 
5. Shut down the communicator(s).

## Let’s get our systems setup

## Basic Communicator Wrangling 

- Managing a Communicator:  Create and destroy communicators
    - `init()` Initialize communicator 
    - `finalize()` shut down communicator(s) 

- Rank query: Determine the processor’s position in the communicator
    -  `comm.rank()` “who am I?” 
    -  `comm.size()` “how many of us are there?” 

- Print: printing with control over which processor prints.
    - `comm.print(x, ...)`
    - `comm.cat(x, ...)`
    
## Simple pbdMIP Example

```r
library(pbdMPI, quietly = TRUE)
init()

myRank <- comm.rank() + 1 # comm index starts at 0, not 1
print(myRank)

finalize()
```
- Use `mpiexec -np 2 Rscript mpiex1.r` to run

---

## Your turn {data-background=#4CDBB7}

> Try to print "Hello,world." on worker 0

---

```r
library(pbdMPI, quietly = TRUE)
init()

myRank <- comm.rank() 
if ( myRank == 0){
  print ("Hello,world.")
}

finalize()
```

## What is the output of this

```r
library(pbdMPI, quietly = TRUE)
init()
comm.print("Hello , world")
comm.print("Hello , world too")
comm.print("Hello again ", all.rank = TRUE, quiet = TRUE)
finalize()
```

---


## MPI Operations

- Reduce
- Gather
- Broadcast
- Barrier

## Reduce

- each processor has a number `x`; add all of them up, find the largest/smallest, ...
- `reduce(x, op=’sum’)` — reduce to one
- `allreduce(x, op=’sum’)` — reduce to all

---

```r
library(pbdMPI, quiet = TRUE)
init()

n <- sample(1:10, size = 1)

sm <- allreduce(n, op = "sum")
comm.print(sm, all.rank = TRUE)


finalize()
```

## Your turn {data-background=#4CDBB7}

> Generate 1000 random samples from uniform(0,1) distribution on each worker, calculate the mean and find the minimul of all workers.

---

```r
library(pbdMPI, quiet = TRUE)
init()

mx <- mean(runif(1000,0,1))

sm <- allreduce(mx, op = "max")
comm.print(sm, all.rank = TRUE)

finalize()
```
## Gather

- Gather: Say each processor has a number. Create a new object on some processor(s)
containing all of those numbers.
    - `gather(x.gbd)` — only one processor gets result
    - `allgather(x.gbd)` — every processor gets result
    

## Your turn {data-background=#4CDBB7}

> Generate 1000 random samples from uniform(0,1) distribution on each worker, calculate the total mean of all samples on worker 0

---

```r
library(pbdMPI, quiet = TRUE)
init()
myRank <- comm.rank() 

x <- runif(1000,0,1)

xx <- gather(x, unlist = TRUE)

if ( myRank == 0){
mean_xx <- mean(xx)
}

comm.print(mean_xx)

finalize()
```


## Broadcast and Barrier

- Broadcast: One processor has a number `x.gbd` that every other processor should also
have.
    - `bcast(x.gbd)`
- Barrier: “computation wall”; no processor can proceed until all processors can proceed.
    - `barrier()`
    
## Example

```r
library(pbdMPI, quiet = TRUE)
init()

if (comm.rank() == 0) {
  x <- matrix(1:4, nrow = 2)
} else {
  x <- NULL
}

y <- bcast(x)

comm.print(y, all.rank = TRUE)
comm.print(x, all.rank = TRUE)

finalize()
```


## Your turn {data-background=#4CDBB7}

> Generate 1000 random samples from uniform(0,1) distribution on each worker, calculate the total mean of all samples on worker 0, send global mean back to all workers and calculate variance of the samples for each worker.

---

```r
library(pbdMPI, quiet = TRUE)
init()
myRank <- comm.rank()

x <- runif(1000, 0, 1)

xx <- gather(x, unlist = TRUE)

if (myRank == 0) {
  mean_xx <- mean(xx)
} else {
  mean_xx <- NULL
}

mean_global <- bcast(mean_xx)

vars <- sum((x - mean_global)^2) / 1000

comm.print(vars, all.rank = TRUE)

finalize()
```


## RNG Seeds

- Random Seeds:
    - `comm.set.seed(seed, diff=FALSE)`: every processor uses the seed seed
    - `comm.set.seed(seed, diff=TRUE)`: every processor uses an independent seed (via `rlecuyer`)



## Your turn {data-background=#4CDBB7}

> Try the same variance calculation example, but use same seed for each worker.

---

```r
library(pbdMPI, quiet = TRUE)
init()
comm.set.seed(12345, diff=FALSE)

myRank <- comm.rank()

x <- runif(1000, 0, 1)

xx <- gather(x, unlist = TRUE)

if (myRank == 0) {
  mean_xx <- mean(xx)
} else {
  mean_xx <- NULL
}

mean_global <- bcast(mean_xx)

vars <- sum((x - mean_global)^2) / 1000

comm.print(vars, all.rank = TRUE)

finalize()
```



## Caution

- Only print *results*, not *computations*
    - `comm.print ( myFunction (x.gbd ))`
- In short, printing stored objects is safe
```r
myResult <- myFunction(x)
comm.print(myResult)
```

## Apply, Lapply, and Sapply

- `pbdApply(X, MARGIN, FUN, ...)` — analogue of apply()
- `pbdLapply(X, FUN, ...)` — analogue of lapply()
- `pbdSapply(X, FUN, ...)` — analogue of sapply()

## Example

```r
library(pbdMPI, quiet = TRUE)
init()

n <- 100
x <- split((1:n) + n * comm.rank(), rep(1:10, each = 10))
sm <- pbdLapply(x, sum)
comm.print(unlist(sm))

finalize()
```

## Your turn {data-background=#4CDBB7}

> Use `babynames` dataset, find average number of baby born by name. save it to a csv file.

---

```
library(pbdMPI, quiet = TRUE)
library(babynames)
init()

x <- split(babynames$n, babynames$name)
sm <- pbdLapply(x, mean)

write.csv(sm, "output.csv")
finalize()
```



## Exercises

1. Write a script that will have each processor randomly take a sample of size 1 of TRUE and FALSE. Have each processor print its result.
2. Modify the script in bbove to determine if any processors sampled TRUE. Do the same to determine if all processors sampled TRUE. In each case, print the result. Compare to the functions comm.all() and comm.any(). 
3. In pbdMPI, there is a parallel sorting function called comm.sort() which is similar to the usual sort() of R. Implement parallel equivalents to the usual order() and rank() of R.


# Stats Examples

## Example 1 -- Monte Carlo for Pi
```r
library(pbdMPI, quiet = TRUE)
init()

N.gbd <- 1000
X.gbd <- matrix(runif (N.gbd * 2) , ncol = 2)
r.gbd <- sum( rowSums (X.gbd ˆ2) <= 1)
ret <- allreduce(c(N.gbd , r.gbd), op = "sum")
PI <- 4 * ret [2] / ret [1]
comm.print (PI)

finalize()
```


## Example 2 --- Sample Mean and Variance

```r
mpi.stat <- function(x.gbd) {
  ### For mean (x).
  N <- allreduce(length(x.gbd), op = "sum")
  bar.x.gbd <- sum(x.gbd / N)

  bar.x <- allreduce(bar.x.gbd, op = "sum")
  ### For var(x).
  s.x.gbd <- sum(x.gbd^2 / (N - 1))
  s.x <- allreduce(s.x.gbd, op = "sum") - bar.x^2 * (N / (N - 1))

  list(mean = bar.x, s = s.x)
} # End of mpi.stat()


```


## Example 3 --- Random Forest

```r
set.seed(seed =123)
n <- nrow(LetterRecognition)

## get train data

n_train <- floor (0.8*n)
i_train <- sample.int(n , n_train) # Use 4/5 of the data to train
train <- LetterRecognition[i_train, ]
test <- LetterRecognition[-i_train , ]

 ## train random forest
my.k <- 500
rf.all <- randomForest(lettr ~ ., train , ntree = my.k ,norm.votes = FALSE)

## predict test data
pred <- predict(rf.all, test )
correct <- sum(pred == test$lettr )
cat(" Proportion Correct:", correct/(n - n_train) , "\n")

```

---

```r
library(randomForest)
library(mlbench)
library(pbdMPI)

init()
data("LetterRecognition")
comm.set.seed(seed =123)
n <- nrow (LetterRecognition)

## get train data

n_train <- floor(0.8*n)
i_train <- sample.int(n , n_train) # Use 4/5 of the data to train
train <- LetterRecognition[i_train, ]
my.test_rows <- get.jid(n-n_train)
test <- LetterRecognition[my.test_rows, ]

## train random forest
comm.set.seed( seed = 1e6*runif(1), diff=TRUE)

my.k <- floor(500/comm.size())
my.rf <- randomForest(lettr ~ ., train , ntree = my.k ,norm.votes = FALSE)


rf.each <- allgather(my.rf)
rf.all <- do.call(combine, rf.each)

## predict test data
pred <- predict(rf.all, test )
correct <- allreduce(sum(pred == test$lettr ))
comm.cat(" Proportion Correct:", correct/(n - n_train) , "\n")

finalize()
```