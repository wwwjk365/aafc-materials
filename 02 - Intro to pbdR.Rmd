---
title: "Introductions to pbdR"
output:
  revealjs::revealjs_presentation:
    theme: simple
    highlight: tango
    center: true
    smart: true
    incremental: true
---

## Batch Execution

- Running a serial R program in batch: 

```r
Rscript my_script.r 

R CMD BATCH my_script.R
```
- Running a parallel (with MPI) R program in batch

```r
mpirun -np 2 Rscript my_par_script.r
```

## Single Program/Multiple Data (SPMD)

- SPMD is arguably the simplest extension of serial programming
- Only one program is written, executed in batch on all processors
- Different processors are autonomous; there is no manager
- Dominant programming model for large machines for 30 years


## Introduction to pbdMPI

- MPI: Standard for managing communications (data and instructions) between different nodes/computers. 
- Implementations: OpenMPI, MPICH2, Cray MPT, ...
- Enables parallelism (via communication) on distributed machines
- Communicator: manages communications between processors 

---

The general process for directly — or indirectly — utilizing MPI in SPMD programs goes something like this: 

1. Initialize communicator(s). 
2. Have each process read in its portion of the data. 
3. Perform computations. 
4. Communicate results. 
5. Shut down the communicator(s).

## Let’s get our systems setup

## Basic Communicator Wrangling 

- Managing a Communicator:  Create and destroy communicators
    - `init()` Initialize communicator 
    - `finalize()` shut down communicator(s) 

- Rank query: Determine the processor’s position in the communicator
    -  `comm.rank()` “who am I?” 
    -  `comm.size()` “how many of us are there?” 

- Print: printing with control over which processor prints.
    - `comm.print(x, ...)`
    - `comm.cat(x, ...)`
    
## Simple pbdMIP Example

```r
library(pbdMPI, quietly = TRUE)
init()

myRank <- comm.rank() + 1 # comm index starts at 0, not 1
print(myRank)

finalize()
```
- Use `mpiexec -np 2 Rscript mpiex1.r` to run

---

## Your turn

- Try to print "Hello,world." on worker 0

```r
library(pbdMPI, quietly = TRUE)
init()

myRank <- comm.rank() 
if ( myRank == 0){
  print ("Hello,world.")
}

finalize()
```

## MPI Operations

- Reduce
- Gather
- Broadcast
- Barrier

## Reduce

- each processor has a number `x`; add all of them up, find the largest/smallest, ...
- `reduce(x, op=’sum’)` — reduce to one
- `allreduce(x, op=’sum’)` — reduce to all


## Gather

- Gather: Say each processor has a number. Create a new object on some processor(s)
containing all of those numbers.
    - `gather(x.gbd)` — only one processor gets result
    - `allgather(x.gbd)` — every processor gets result
    
## Broadcast and Barrier

- Broadcast: One processor has a number `x.gbd` that every other processor should also
have.
    - `bcast(x.gbd)`
- Barrier: “computation wall”; no processor can proceed until all processors can proceed.
    - `barrier()`
    

## Example 2

```r
library(pbdMPI, quiet = TRUE)
init()

n <- sample(1:10, size = 1)
gt <- gather(n)
comm.print(unlist(gt))

sm <- allreduce(n, op = "sum")
comm.print(sm, all.rank = TRUE)


finalize()
```
- try run this example multiple times

---

```r
library(pbdMPI, quiet = TRUE)
init()

if (comm.rank() == 0) {
  x <- matrix(1:4, nrow = 2)
} else {
  x <- NULL
}

y <- bcast(x)

comm.print(y, all.rank = TRUE)
comm.print(x, all.rank = TRUE)

finalize()
```
## RNG Seeds

- Random Seeds:
    - `comm.set.seed(seed, diff=FALSE)`: every processor uses the seed seed
    - `comm.set.seed(seed, diff=TRUE)`: every processor uses an independent seed (via `rlecuyer`)

---

```r
library(pbdMPI, quietly = TRUE)
init()

myRank <- comm.rank() 

comm.print("Hello, world")

finalize()
```

## Caution

- Only print *results*, not *computations*
- In short, printing stored objects is safe


## Apply, Lapply, and Sapply

- `pbdApply(X, MARGIN, FUN, ...)` — analogue of apply()
- `pbdLapply(X, FUN, ...)` — analogue of lapply()
- `pbdSapply(X, FUN, ...)` — analogue of sapply()

---

example

```r
library(pbdMPI, quiet = TRUE)
init()

n <- 100
x <- split((1:n) + n * comm.rank(), rep(1:10, each = 10))
sm <- pbdLapply(x, sum)
comm.print(unlist(sm))

finalize()
```

## Exercises

1. Write a script that will have each processor randomly take a sample of size 1 of TRUE and FALSE. Have each processor print its result.
2. Modify the script in Exercise 3-1 above to determine if any processors sampled TRUE. Do the same to determine if all processors sampled TRUE. In each case, print the result. Compare to the functions comm.all() and comm.any(). 
3. In pbdMPI, there is a parallel sorting function called comm.sort() which is similar to the usual sort() of R. Implement parallel equivalents to the usual order() and rank() of R.


# Stats Examples

## Example 1 -- Monte Carlo for Pi
```r
library(pbdMPI, quiet = TRUE)
init()

N.gbd <- 1000
X.gbd <- matrix(runif (N.gbd * 2) , ncol = 2)
r.gbd <- sum( rowSums (X.gbd ˆ2) <= 1)
ret <- allreduce(c(N.gbd , r.gbd), op = "sum")
PI <- 4 * ret [2] / ret [1]
comm.print (PI)

finalize()
```



## Example 2 --- Sample Mean and Variance

```r
mpi.stat <- function(x.gbd) {
  ### For mean (x).
  N <- allreduce(length(x.gbd), op = "sum")
  bar.x.gbd <- sum(x.gbd / N)

  bar.x <- allreduce(bar.x.gbd, op = "sum")
  ### For var(x).
  s.x.gbd <- sum(x.gbd^2 / (N - 1))
  s.x <- allreduce(s.x.gbd, op = "sum") - bar.x^2 * (N / (N - 1))

  list(mean = bar.x, s = s.x)
} # End of mpi.stat ()


```


## Example 3 --- Random Forest

```r
set.seed (seed =123)
n <- nrow (LetterRecognition)

## get train data

n_train <- floor (0.8*n)
i_train <- sample.int(n , n_train) # Use 4/5 of the data to train
train <- LetterRecognition[i_train, ]
test <- LetterRecognition[-i_train , ]

 ## train random forest
my.k <- 500
rf.all <- randomForest(lettr ~ ., train , ntree = my.k ,norm.votes = FALSE)

## predict test data
pred <- predict(rf.all, test )
correct <- sum(pred == test$lettr )
cat(" Proportion Correct:", correct/(n - n_train) , "\n")

```

---

```r
library(randomForest)
library(mlbench)
library(pbdMPI)

init()
data("LetterRecognition")
comm.set.seed(seed =123)
n <- nrow (LetterRecognition)

## get train data

n_train <- floor(0.8*n)
i_train <- sample.int(n , n_train) # Use 4/5 of the data to train
train <- LetterRecognition[i_train, ]
my.test_rows <- get.jid(n-n_train)
test <- LetterRecognition[my.test_rows, ]

## train random forest
comm.set.seed( seed = 1e6*runif(1), diff=TRUE)

my.k <- floor(500/comm.size())
my.rf <- randomForest(lettr ~ ., train , ntree = my.k ,norm.votes = FALSE)


rf.each <- allgather(my.rf)
rf.all <- do.call(combine, rf.each)

## predict test data
pred <- predict(rf.all, test )
correct <- allreduce(sum(pred == test$lettr ))
comm.cat(" Proportion Correct:", correct/(n - n_train) , "\n")

finalize()
```